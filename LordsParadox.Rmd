Lord's Paradox
==============

```{r Setup, echo=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, include = FALSE, message = FALSE, 
					  warning = FALSE)
library(tidyverse)
library(emmeans)
library(car)
library(patchwork)
theme_set(theme_bw())
cols = c('dodgerblue2','goldenrod3')
a_mean = 150
b_mean = 160
set.seed(4)
a_0 = rnorm(500, a_mean, 4)
b_0 = rnorm(500, b_mean, 4)
delt = 0.3
set.seed(5)
a_1 = delt*a_mean + (1-delt)*a_0 + rnorm(500, 0, 1.5)
b_1 = delt*b_mean + (1-delt)*b_0 + rnorm(500, 0, 1.5)

l_df = data.frame(Dorm = rep(c('B','A'), each = 500),
				Initial = c(b_0,a_0),
				Final = c(b_1,a_1),
				Gain = c(b_1,a_1) - c(b_0,a_0))
head(l_df)

l_df_long = (l_df
		   %>% select(-Gain)
		   %>% gather(Timepoint, Weight, Initial, Final))
head(l_df_long)
```

## Lord's Formulation

* A university has two dormitories with two different cafeteria menus. We want
to know:
	* How do students' weights change over the course of their first year at 
	university?
	* What is the effect of the different menus on student weight change?
	
* Students in the two dormitories are weighed at the beginning and end of the 
school year:

```{r Scatter, include = TRUE}
ggplot(l_df, aes(Initial, Final)) +
	geom_point(aes(colour = Dorm), alpha = 0.3) +
	stat_ellipse(aes(colour = Dorm)) +
	# geom_smooth(aes(colour = Dorm), method = 'lm',
	# 			se = FALSE) +
	# geom_smooth(method = 'lm', se = FALSE, colour = 'black') +
	geom_abline(intercept = 0, slope = 1, linetype = 2) +
	scale_colour_manual(values = cols)

```

* Blue points/ellipse represent students in Dorm B, gold represent Dorm A.
* The dashed line is the diagonal: slope of 1 and intercept of 0.

Two statisticians analyze these data. 

### Anova

* The first compares the mean and variance of the initial vs. final weights in 
each of the two dorms
	* In Dorm A, the initial weights have the same mean and variance as the 
	final weights
	* Similarly in Dorm B, the initial and final weights have the same 
	distribution
	* This statistician concludes that, while any individual student's weight 
	might change substantially over the course of the year, the gains and losses
	cancel each other and any overall effect is negligible.

It was 1967, so Lord used ANOVAs:

```{r ANOVA, echo = TRUE, include = TRUE}
l_a_anov = aov(Weight ~ Timepoint, filter(l_df_long, Dorm == 'A'))
l_b_anov = aov(Weight ~ Timepoint, filter(l_df_long, Dorm == 'B'))
Anova(l_a_anov)
Anova(l_b_anov)
```

```{r ANOVPlt, include = TRUE}
ggplot(l_df_long, aes(Weight)) +
	geom_histogram() +
	facet_grid(Timepoint ~ Dorm)
```

But we can do an equivalent thing with a linear model:

```{r LM1, echo = TRUE, include = TRUE}
l_mod1 = lm(Gain ~ Dorm, data = l_df)
summary(l_mod1)
l_em_df = data.frame(emmeans(l_mod1, 'Dorm'))
```

```{r Boxplot, include = TRUE}
ggplot(l_df, aes(Dorm, Gain)) +
	geom_jitter(aes(colour = Dorm),
				alpha = 0.3, 
				width = 0.3) +
	geom_boxplot(fill = NA) +
	geom_point(data = l_em_df, aes(y = emmean),
			   size = 3, alpha = 0.5) +
	scale_colour_manual(values = cols)
```

### ANCOVA

* The second statistician decides to control for the initial weight of the 
students
	* The slopes of the regression lines between initial and final weights are
	similar in the two dorms, which allows them to compare the intercepts with
	an ANCOVA:

```{r ANCOVA, echo = TRUE, include = TRUE}
l_ancov = aov(Final ~ Dorm + Initial, data = l_df)
Anova(l_ancov)
```

Again, we can do an equivalent test with a linear model

```{r LM2, include = TRUE, echo=TRUE}
l_mod2 = lm(Final ~ Dorm + Initial, data = l_df)
summary(l_mod2)

summary(lm(Gain ~ Dorm + Initial, data = l_df))
```

And indeed, it is clear from the graph that the two regression lines will have
very different intercepts:

```{r LordPlt, include = TRUE}
ggplot(l_df, aes(Initial, Final)) +
	geom_point(aes(colour = Dorm), alpha = 0.3) +
	stat_ellipse(aes(colour = Dorm)) +
	geom_smooth(aes(colour = Dorm), method = 'lm',
				se = FALSE) +
	geom_abline(intercept = 0, slope = 1, linetype = 2) +
	scale_colour_manual(values = cols)
```

In other words: Students in Dorm B are gaining more weight than students in 
Dorm B because _for any given initial weight_, the average final weight in Dorm
B is higher than Dorm A.

So, what is going on here?

## A Digression: Mediation

Let's construct a different data set. In this data set students in Dorm B will
gain more weight than students in Dorm B, and this gain will be _mediated_ by
initial weight. I.e., the more you weight initially, the more you gain.

```{r Mediation, include=TRUE, echo=TRUE}
# Start with 1000 students
m_a_0 = rnorm(500, 150, 4)
m_b_0 = rnorm(500, 160, 4)

# Final weight (and therefore weight gain) solely a function of initial weight
m_a_f = (1+delt)*m_a_0 + rnorm(500, 0, 1.5)
m_b_f = (1+delt)*m_b_0 + rnorm(500, 0, 1.5)

m_df = data.frame(Dorm = rep(c('A','B'), each = 500),
				Initial = c(m_a_0, m_b_0),
				Final = c(m_a_f, m_b_f),
				Gain = c(m_a_f - m_a_0, m_b_f - m_b_0))
head(m_df)
m_df_long = (m_df
		   %>% select(-Gain)
		   %>% gather(Timepoint, Weight, Initial, Final))

ggplot(m_df, aes(Initial, Final)) +
	geom_point(aes(colour = Dorm), alpha = 0.3) +
	stat_ellipse(aes(colour = Dorm)) +
	# geom_smooth(aes(colour = Dorm), method = 'lm',
	# 			se = FALSE) +
	geom_abline(intercept = 0, slope = 1, linetype = 2) +
	scale_colour_manual(values = cols)
```

In this case, both groups of students are gaining weight, but that gain is
only related to the initial weight. There is no direct effect of Dorm.

### ANOVA

Let's do what Lord's first hypothetical statistician did, and run an ANOVA:

```{r ANOVAmed, include=TRUE, echo=TRUE}
m_a_anov = aov(Weight ~ Timepoint, filter(m_df_long, Dorm == 'A'))
m_b_anov = aov(Weight ~ Timepoint, filter(m_df_long, Dorm == 'B'))
Anova(m_a_anov)
Anova(m_b_anov)
```

There is a clear effect of Dorm, even though we did not include one when we 
constructed the data set.

Looking at the distributions, it's clear their location has moved:

```{r ANOVMedPlt, include = TRUE}
ggplot(m_df_long, aes(Weight)) +
	geom_histogram() +
	facet_grid(Timepoint ~ Dorm)
```

And here is the equivalent linear model:

```{r LM1med, echo = TRUE, include = TRUE}
m_mod1 = lm(Gain ~ Dorm, data = m_df)
summary(m_mod1)
m_em_df = data.frame(emmeans(m_mod1, 'Dorm'))
```

```{r Boxplotmed, include = TRUE}
ggplot(m_df, aes(Dorm, Gain)) +
	geom_jitter(aes(colour = Dorm),
				alpha = 0.3, 
				width = 0.3) +
	geom_boxplot(fill = NA) +
	geom_point(data = m_em_df, aes(y = emmean),
			   size = 3, alpha = 0.5) +
	scale_colour_manual(values = cols)
```

When we don't control for initial weight, we see a clear effect of Dorm.

### ANCOVA

Now let's follow Lord's second statistician's method. Again. the slopes in the
two groups are similar, so we are justified in comparing their intercepts.

```{r ANCOVAmed, echo = TRUE, include = TRUE}
m_ancov = aov(Gain ~ Dorm + Initial, data = m_df)
Anova(m_ancov)
```

Effect of Dorm is no longer significant!

Linear model framework:

```{r LM2med, include = TRUE, echo=TRUE}
m_mod2 = lm(Gain ~ Dorm + Initial, data = m_df)
summary(m_mod2)
```

The intercept and effect of Dorm have both disappeared. Indeed, the intercepts
of the two regression lines are the same.

```{r LordPltmed, include = TRUE}
ggplot(m_df, aes(Initial, Final)) +
	geom_point(aes(colour = Dorm), alpha = 0.3) +
	stat_ellipse(aes(colour = Dorm)) +
	geom_smooth(aes(colour = Dorm), method = 'lm',
				se = FALSE) +
	geom_abline(intercept = 0, slope = 1, linetype = 2) +
	scale_colour_manual(values = cols)
```

So, what happened here?  Who is correct?

### Everyone is right; context is everything

In this toy example, it is clear that Dorm is a red herring, but that is less 
obvious in real life examples:

* Poverty, toxins, cancer
	* what can you measure?
	* what info can clinicians access?
	* what policies are you trying to inform?
	
The fact that the effect of X on Y is _mediated_ by M doesn't mean that the 
effect is not real, or not important. Nor does it mean that M doesn't matter. 
It's all real.

## Connect Mediation back to Lord

Lord's Paradox is really a case of anti-mediation.

In both cases, Dorm is a positive function of $W_I$ (or vice versa)

$$
D \sim \beta_{30} + \beta_{31} * W_I
$$

```{r, include = TRUE}
df_all = (rbind((l_df
				%>% mutate(DataSet = 'Lord')),
				(m_df
				 %>% mutate(DataSet = 'Mediation')))
		  %>% mutate(DataSet = factor(DataSet, levels = c('Mediation','Lord'))))

d_i = ggplot(df_all, aes(Dorm, Initial)) +
	geom_boxplot(aes(fill = Dorm)) +
	scale_fill_manual(values = cols) +
	facet_wrap(~DataSet)
d_i
```

In both cases, I constructed gain purely as a function of $W_I$. 
The only difference was the sign.

$$
G \sim \beta_{20} + \beta_{21} * W_I
$$

```{r, include = TRUE}
g_i = ggplot(df_all, aes(Initial, Gain)) +
	geom_point(aes(colour = Dorm), alpha = 0.3) +
	stat_ellipse(aes(colour = Dorm)) +
	# geom_smooth(aes(colour = Dorm), method = 'lm',
	# 			se = FALSE) +
	# geom_smooth(method = 'lm', se = FALSE, colour = 'black') +
	scale_colour_manual(values = cols) +
	facet_wrap(~DataSet)
g_i
```

Which is what leads us to our different conclusions.

$$
G \sim \beta_{10} + \beta_{11} * D
$$

```{r, include = TRUE}
d_g = ggplot(df_all, aes(Dorm, Gain)) +
	geom_boxplot(aes(fill = Dorm)) +
	scale_fill_manual(values = cols) +
	facet_wrap(~DataSet)
d_g
```


There is no need for the mediating variable to be the baseline value of the 
response variable. E.g.

* Sex
* Body fat % (not actually linear but never mind)
* Cardiovascular disease

## References
Most of this is borrowed from [Judea Pearl's 
explanation](https://ftp.cs.ucla.edu/pub/stat_ser/r436.pdf) of the problem.